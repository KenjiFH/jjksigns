# RFC: Geometric Hand Gesture Recognition System

**Status:** Proposed
**Author:** [Kenji F.]

## Context
Current naive gesture recognition systems operating on raw RGB frames suffer from significant environmental sensitivity. They exhibit high false-positive rates due to lighting changes, background clutter, and camera variations. Additionally, per-frame classification accuracy does not translate directly to a usable user interface experience; "jittery" predictions create a poor user experience.

We require a system that prioritizes **intentionality** and **stability** over raw per-frame accuracy. The system must be camera-agnostic and capable of recognizing custom static and quasi-static configurations (single and two-handed) without requiring end-to-end deep learning on RGB data.

## Decision
We will implement a **geometry-based decision system** rather than an end-to-end vision classifier. The system will decouple offline feature engineering/training from online runtime inference.

### 1. Architecture Pipeline
The runtime pipeline is strictly defined as follows:
1.  **Input:** Raw Camera Frames.
2.  **Extraction:** MediaPipe Hands (yielding 3D landmarks).
3.  **Feature Engineering:** Normalization, landmark-to-landmark geometry, and cross-hand metrics (merged 129-float feature vector).
4.  **Classification:** Scikit-learn **SVM (RBF Kernel)**.
5.  **Confidence:** Probability estimation (Softmax-equivalent).
6.  **Decision Logic:** Hysteresis filtering, temporal stability checks, and cooldown enforcement.
7.  **Output:** Discrete Gesture Intent.

### 2. Feature Representation
* **Geometry over Pixels:** We reject raw pixel processing. All classification relies exclusively on normalized hand landmarks.
* **Unified Pose Vector:** Left and right hands are not classified independently. They are merged into a single feature vector to support complex two-handed configurations (e.g., *Domain Expansion* gestures).
* **Explicit Rejection:** The model must include a specific `None`/`Unknown` class. We do not force a prediction if the input does not match a known geometric configuration.

### 3. Model & Training
* **Model Selection:** We select a Support Vector Machine (RBF Kernel) over a Multi-Layer Perceptron (MLP). The SVM provides sufficient non-linear separation for geometric clusters with lower training complexity and clearer decision boundaries for this dataset size.
* **Data Strategy:** Training data will be recorded as sessions (video), not isolated frames. Samples will be generated by aggregating short, stable frame windows.
* **Offline/Runtime Separation:**
    * *Training (Jupyter):* Dataset cleaning, feature validation, and model weight export.
    * *Runtime (Python App):* Strictly inference and state management. No training logic exists in the deployment artifact.

### 4. Decision Logic (Post-Processing)
Raw model probability is insufficient for triggering actions. The system must implement:
* **Confidence Thresholds:** $P(gesture) > 0.9$.
* **Temporal Consistency:** The classification must hold for $N$ consecutive frames.
* **Cooldowns:** A successful trigger forces a lockout period to prevent double-firing.

## Alternatives Considered

### End-to-End RGB CNNs
* **Status:** Rejected.
* **Reasoning:** High fragility regarding lighting and background; requires massive datasets to generalize across different environments; high computational cost compared to geometric analysis.

### Template Matching (Geometric heuristics)
* **Status:** Rejected.
* **Reasoning:** Poor scalability. Hard-coding geometric rules (e.g., "thumb tip distance from index tip") for every new gesture is unmaintainable and brittle compared to learning boundaries via SVM.

### Temporal Deep Models (LSTM/Transformer)
* **Status:** Rejected.
* **Reasoning:** Unnecessary complexity. The target gestures are static or quasi-static. Temporal stability can be handled more efficiently via deterministic logic (sliding windows) rather than probabilistic temporal modeling.

## Rationale
1.  **Robustness:** MediaPipe provides an abstraction layer that solves the lighting/background problem. By classifying the *skeleton*, we ignore the *environment*.
2.  **Intentionality:** The combination of an explicit `None` class and temporal hysteresis ensures that users must deliberately perform a gesture to trigger an action, eliminating "twitchy" false positives.
3.  **Maintainability:** Separating the Jupyter training environment from the Python runtime ensures the deployed application remains lightweight and deterministic.

## Consequences

### Positive
* **Stability:** The system will resist environmental noise that breaks RGB-based models.
* **Performance:** Geometric feature extraction and SVM inference are computationally negligible compared to video processing, leaving headroom for the application logic.
* **Extensibility:** New gestures can be added by recording a new session and re-running the offline training pipeline without code changes to the runtime.

### Negative
* **Dependency Risk:** The system creates a hard dependency on MediaPipe's ability to detect hands. If MediaPipe fails (e.g., extreme occlusion or gloves), the downstream classifier cannot recover.
* **Texture Loss:** We lose all texture information. Gestures that rely on visual cues rather than geometry (e.g., color, skin texture) are impossible to detect.
* **Latency:** The requirement for temporal consistency (waiting $N$ frames to confirm a gesture) introduces a small, deliberate input lag (approx. 100-200ms).
