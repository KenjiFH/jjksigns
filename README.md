# Domain Expansion: Real-Time Gesture Recognition with MediaPipe & SVMs

### A Computer Vision & HCI Project inspired by Jujutsu Kaisen


## ğŸ“– Overview
This project is a real-time gesture recognition engine that triggers visual effects (like *Domain Expansions* from JJK) based on specific hand signs. 

Instead of using heavy Deep Learning models, this project demonstrates an efficient **Machine Learning Pipeline** using **Google MediaPipe** for landmark extraction and a **Support Vector Machine (SVM)** for classification. This approach ensures low latency and high accuracy even on standard CPU hardware.

## âœ¨ Features
* **Real-time Hand Tracking:** Uses MediaPipe to track 21 3D landmarks per hand.
* **Custom ML Pipeline:** Features a built-in data collection mode to create your own datasets.
* **Scale Invariance:** Custom vector normalization logic ensures gestures work at any distance from the camera.
* **Visual Effects engine:** Triggers complex overlays (Infinite Void, Malevolent Shrine) upon high-confidence detection.
* **HCI Principles:** Implements hysteresis and confidence thresholds to prevent UI flickering.

## ğŸ› ï¸ Tech Stack
* **Python 3.11**
* **OpenCV:** Video capture and visual overlay rendering.
* **MediaPipe:** Hand landmark detection.
* **Scikit-Learn:** SVM (RBF Kernel) for gesture classification.
* **NumPy:** Vector math and normalization.

## ğŸš€ Installation

1.  **Clone the repo**
    ```bash
    git clone [https://github.com/KenjiFH/jjksigns.git](https://github.com/KenjiFH/jjksigns.git)
    cd jjksigns
    ```

2.  **Install Dependencies**
    ```bash
    pip install -r requirements.txt
    ```

3.  **Run the Application**
    ```bash
    python src/camera_input.py
    ```

## ğŸ® Controls & Usage

| Key | Action |
| :--- | :--- |
| **ESC** | Quit the application |
| **TAB** | Cycle through available gesture labels (for recording,  |
| **Hold 'C'** | Record training data for the selected label, 2 handed gestures are automatically recorded so no need for a third hand |

### Creating Your Own Gestures (The Pipeline)
This project includes a full training loop. To add a new gesture:

1.  Run the app.
2.  Press `TAB` to select a label (or add a new one in `CLASS_NAMES` in the code).
3.  Hold **'C'** while performing the gesture to capture ~50-100 frames of data.
4.  The data is saved to `models/keypoint_classifier/keypoints.csv`.
5.  Open `notebooks/train_model.ipynb` and run the cells to retrain the SVM.
6.  The new model will be saved as `gesture_model.pkl`. Restart the app to use it!

## ğŸ“‚ Project Structure
```text
jjksigns/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ hand_landmarker.task   # MediaPipe base model
â”‚   â””â”€â”€ keypoint_classifier/   # CSV datasets
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ gesture_model.pkl      # Trained SVM model
â”‚   â””â”€â”€ train_model.ipynb      # Training workflow
â”œâ”€â”€ src/
â”‚   â””â”€â”€ camera_input.py        # Main application entry point
â””â”€â”€ requirements.txt
